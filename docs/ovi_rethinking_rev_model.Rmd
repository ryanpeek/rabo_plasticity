---
title: "Oviposition Logistic"
author: "Ryan Peek"
date: "Updated: `r format(Sys.Date())`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("../"))
```

## What Permits Plasticity in Breeding Timing in River Frogs?

How might varying environmental conditions provide cues for breeding timing from year to year? For frogs that have an amazing plasticity in breeding oviposition timing (inital deposition of eggs) such as *Rana boylii*, there must be certain thresholds that exist (a cutoff which makes oviposition not possible, physiologically). However, beyond a given threshold, there also may be certain cues which act as important environmental markers in dynamic environments such as rivers. By utilizing both, it is likely that the odds of reproductive success improve, and over time this results in evolutionary success. For example, frogs that breed to soon are more likely to have eggs scoured out by late spring storms or have longer developmental/hatching times because water temperatures are lower. Frogs breeding later in the season may trade off greater egg deposition and hatching success with later metamorphosis and lower overwinter survival. 

We know that frogs from the same populations in the exact same locations can deposit eggs more than a month later or many weeks earlier than the average observed date of deposition, depending on the annual conditions (i.e., wetter or drier years). We suspect that the water temperature may be the most important driver in this plasticity, as it may act as a threshold which once exceeded, provides a very strong indicator of "summer" or future hydrologic condition.

However, the flow in a given reach of river can also be highly correlated with water temperature. Linkage between water temperature and patterns in flow may be tightly tied, particularly in rivers in the Sierra Nevada. Historically, Sierra Nevada river hydrology has been driven by a general Mediterranean pattern of wet (though variable) winter weather followed by spring snowmelt and warmer and drier summers. While temperature may exist as a spawning threshold, the actual initiation of breeding or spawning is highly variable in **R. boylii**, and likely requires additional enviromental cues such as changing water levels, flow magnitude, air temperature, humidity, etc.

These additional metrics may each act as environmental cues, but to conserve plasticity across environmental variablity present in a wide geographic range, it is unlikely that any of these can dictate oviposition independently (whereas a water temperature threshold may be a sole driver in some years). In order to determine the strongest predictor of breeding, we've collected oviposition data over 6 rivers in the Sierra Nevada from 2011 through 2016. In addition we collected a variety of metrics related to flow, air temperature, water temperature, water year index, day of water year, precipitation, and barometric pressure.

## Try Model, Model Fail

Let's try to assess whether water temperature, air temperature, flow (daily/weekly recession rates & discharge) play a role in predicting when breeding (oviposition occurs). To build this model we need to block by river, and by water year, as these are independent (though water year isn't *fixed*, it is fixed across all sites per each year).

First let's take a look at some test data. Using a subset (say just pull the **NF American**), can we build a model that gets at whether there is predictability in these variables despite shifts in timing and magnitude from year to year? Ideally this seems to be setup for a logistic regression, where 1 is the breeding initation or observation of eggs in the river, and 0 is all the days prior to that event. An *Event History Analysis* or *Survival Analysis* approach might work (see section on geometric distribution, Ch10, pg 328). 

### Get the Data & Tidy

```{r get and tidy data, eval=F, echo=F}

library(tidyverse)
library(lubridate)

load("data/master_dat_2011-2016.rda") 

# fill the REG col across all rows
#master_df$REG <- ifelse(master_df$site %in% c("MFA","MFY","RUB","SFY"), "R","U")

load("data/flow_dv_cfs_2011_6sites.rda") # updated and merged flows:

df <- master_df
# df <- master_df %>% filter(site!="MFY")
# df <- master_df %>% filter(month(date)>2 & month(date)<8 & !site=="MFA")
# df <- master_df %>% filter(month(date)>3 & month(date)<8 & site=="NFA" | site=="NFY")


# 2 NAs in dataset from 30 day avgs
df[!is.na(df$missData) & df$site=="RUB" & df$date==ymd("2011-05-26"),]$temp_30_max<-10.99
df[!is.na(df$missData) & df$site=="RUB" & df$date==ymd("2011-05-26"),]$temp_30_min<-9.7

rabo <- df[!is.na(df$missData),] %>% arrange(date)

# add unique rownames based on siteID
rabo <- rabo %>%
  mutate("siteID" = paste0(site, "-", row.names(.))) %>% 
  column_to_rownames(var = "siteID")  %>% as.data.frame
#names(rabo)
#rownames(rabo)

# create column with date for 15 and 30 day lags (for binomial logistic)
rabo_lag <- rabo %>% 
  mutate(d07 = date - 7, 
         d14 = date - 14,
         d21 = date - 21,
         d30 = date - 30) %>% 
  select(site, date, WY, d07:d30)

# rejoin / filter form orig dataset

rabo_bin7<- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d07")) %>% select(-c(date.y, d14:d30)) %>% mutate(lagEM="d07")
rabo_bin14 <- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d14")) %>% select(-c(date.y, d07:d30)) %>% mutate(lagEM="d14")
rabo_bin21 <- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d21")) %>% select(-c(date.y, d07:d30)) %>% mutate(lagEM="d21")
rabo_bin30 <- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d30")) %>% select(-c(date.y, d07:d21)) %>% mutate(lagEM="d30")

# bind up for one dataset of lags, add 1/0 col for breeding
rabo_LAGS<-rbind(rabo_bin7, rabo_bin14, rabo_bin21, rabo_bin30) %>% 
  mutate(breed = 0)
rm(list=ls(pattern = "rabo_bin*"))

# add breed col to orig dataset:
rabo <- rabo %>% mutate(breed = 1, lagEM="d00")

# make final logistic dataset, add 1/0 column
rabo_logist <- rbind(rabo_LAGS, rabo)

# fill the REG col across all rows
#rabo_logist$REG <- ifelse(rabo_logist$site %in% c("MFA","MFY","RUB","SFY"), "R","U")

#save(rabo_logist, file = "models/rabo_logistic_all_sites.rda")
```

Prepare data by removing much of the information that will be unused, and converting and scaling the data as needed.

```{r prepData, eval=T, echo=T}

library(tidyverse)
library(lubridate)
library(rethinking)

# custom scale function
cusScale <- function(x){
  (x - mean(x))/sd(x)
}

unScale <- function(x, y){
  (x * sd(y) + mean(y))
}

# LOAD DATA
load("models/rabo_logistic_all_sites.rda")

# UNREG: Filter down data, use only unreg sites
d1 <- dplyr::select(rabo_logist, site, breed, lagEM, DOWY, everything(), 
                    -date, -EM_per_km, -REG, -(obs_strt:apr_jul),
                    -len_km, -starts_with("CDEC"), -station, -DOY,
                    -WYsum, -lev_7_avg, -Q_cfs, -Q_min, -Q_7_cfs) %>% 
    #filter(!site=="MFA") %>% 
  filter(site=="NFY" | site=="NFA") %>%
  as.data.frame

# RM NAs, add id and site to rownames:
d1 <- d1 %>% filter(!is.na(temp_30_min))
#d1_rownames<- paste0(d1$site, "-",d1$WY, "-", seq(1:nrow(d1)))
d1$site <- as.factor(d1$site)
d1$lagEM <- as.factor(d1$lagEM)

# scale all data and rebind site and breed info:
d1s <- d1
d1s[,c(4:30)] <- apply(d1[,c(4:30)], 2, cusScale) # center and scale
d1s$site <- coerce_index(d1s$site) # for modeling
#d1s$WY <- coerce_index(d1s$WY) # for modeling
d1s$lagEM <- coerce_index(d1s$lagEM) # for modeling
#d1s <- d1s %>% rename(river=site, wyind=Index) # rename some vars

# update cols
d1$Qmx_log <- log(d1$Q_max)
d1$DOWYprop <- d1$DOWY/365 # poisson can't have negatives
d1s$DOWY <- d1$DOWY
d1$deltLevEX<-exp(d1$deltLev)
d1$lagEM<-coerce_index(d1$lagEM)
d1$obs <- 1:nrow(d1)
d1$river<-coerce_index(d1$site)

# only the breeding points?
#d1 <- d1 %>% filter(breed==1)
#d1s <- d1s %>% filter(breed==1)


# ALL DATA: Filter down data, use only reg sites
d2 <- dplyr::select(rabo_logist, site, breed, lagEM, DOWY, everything(), 
                    -date, -EM_per_km, -(obs_strt:apr_jul),
                    -len_km, -starts_with("CDEC"), -station, -DOY,
                    -WYsum, -lev_7_avg, -Q_cfs, -Q_min, -Q_7_cfs) %>% 
  filter(!site=="MFA", !site=="MFY") %>% #, !site=="NFY", !site=="NFA") %>%
  as.data.frame

# RM NAs, add id and site to rownames:
d2 <- d2 %>% filter(!is.na(temp_30_min)) #rm the NAs
d2$site <- factor(d2$site)
d2$REG_id <-ifelse(d2$REG=="R",1,0)
d2$lagEM <- as.factor(d2$lagEM)
d2 <- d2 %>% select(-REG)

# scale all data and rebind site and breed info:
d2s <- d2
d2s[,c(5:30)] <- apply(d2[,c(5:30)], 2, cusScale) # center and scale
d2s$site <- coerce_index(d2s$site) # for modeling
d2s$lagEM <- coerce_index(d2s$lagEM) # for modeling

# update cols
d2$Qmx_log <- log(d2$Q_max)
d2$DOWYprop <- (d2$DOWY)/365
d2s$DOWYprop <- (d2s$DOWY)/365
d2s$DOWY <- d2$DOWY
d2$deltLevEX<-exp(d2$deltLev)
d2$lagEM<-coerce_index(d2$lagEM)
d2$obs <- 1:nrow(d2)
d2$river<-coerce_index(d2$site)
d2s$WY_id <- coerce_index(d2$WY)

# only the breeding points?
d2b <- d2 %>% filter(breed==1)
d2sb <- d2s %>% filter(breed==1)
d2sb$DOWYprop <- (d2sb$DOWY)/365

# some date strings
dowy_labs<-c("Oct","Nov","Dec","Jan", "Feb","Mar", "Mar-15", "Apr-01", "Apr-15", "May-01", "May-15", "Jun-01", "Jun-15", "Jul-01", "Jul-15", "Aug", "Aug-15", "Sep")
dowy_breaks<-c(1, 32, 62, 93, 124, 152, 167, 183, 198, 213, 228, 244, 259, 274, 289, 305, 320, 336)
dowys<-data.frame("mon"=dowy_labs, "dowy"=dowy_breaks)

# look up for DOWY
# dowy_all<-master_df %>% filter(WY==2013, site=="NFY") %>% 
#   select(DOWY, date) %>% 
#   mutate(mon=month(date, label = T, abbr = T),
#          mday=mday(date),
#          monDay = paste0(mon, "-", mday)) %>% select(DOWY, monDay)
# dowy_all
#save(dowy_all, file = "data/dowy_lookup.rda")
load("data/dowy_lookup.rda")

```

### Do Some Exploratory Modeling

After initially trying logistic regression, I think the better approach may be using a continuous variable (DOWY), and including breeding as a binary 1/0, or just use the breeding data. It would allow a better sense of how relevant the effect is for spawning date.

#### Poisson Models

Poisson models (or a *Poisson process*) needs to meet the following criteria:

 - The probability of at least one occurrence of the event in a given time
interval is proportional to the length of the interval.
 - The probability of two or more occurrences of the event in a very small
time interval is negligible.
 - The numbers of occurrences of the event in disjoint time intervals are
mutually independent.

Then the probability distribution of the number of occurrences of the event
in a fixed time interval is Poisson with mean $\mu = \lambda t$, where $\lambda$ is the rate of occurrence of the event per unit of time and t is the length of the time interval.

Following these criteria, the day of egg deposition, or breeding initiation, would be a suitable metric for a Poisson process model.  A Poisson model tends to do well when the variance tends to increase with the mean.

```{r poisson1, eval=F, echo=F}
# response has to be positive integer, so use DOWY lag as metric.

# breeding lag
mP1<- map2stan(
  alist(
    lagEM ~ dpois(mu),
    log(mu) <- a + a_wy[WY_id] +
      bt7mx*temp_7_max + bt30mn*temp_30_min + btCV*temp_CV + # water temp
      bairmin*W_air_min + bairmax*W_air_max + bppt*days_no_ppt + bwyi*Index + #air/ppt
      bQCV*Q_CV + bdLev*deltLev + blevavg*lev_avg + bdQ*deltQ + # flow
      bReg*REG_id, #regulation
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bt7mx, bt30mn, btCV, bairmin, bairmax, bppt, bwyi,
      bQCV, bdLev, blevavg, bdQ, bReg) ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
),
data=d2s, iter=1e4, chains=3, cores=2)


# plots and stuff
plot(mP1)
dev.off()
#pairs(mP1)
precis(mP1, depth=2, warn=F)
plot(precis(mP1, depth=2,warn=F))


# Model P2: Varying Intercept for WY, breeding only, all sites
mP2<- map2stan(
  alist(
    lagEM ~ dpois(mu),
    log(mu) <- a + a_wy[WY_id] + bt7mx*temp_7_max + bt30mn*temp_30_min + bQCV*Q_CV + bdLev*deltLev + bwyi*Index + bdQ*deltQ + bReg*REG_id + btCV*temp_CV,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    bLEM ~ dnorm(2, 10),
    c(bt7mx, bt30mn, bQCV, bdLev, bwyi, bdQ, bReg, btCV) ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
),
data=d2s, iter=1e4, chains=3, cores=2)


# plots and stuff
plot(mP1)
dev.off()
#pairs(mP1)
precis(mP1, depth=2, warn=F)
plot(precis(mP1, pars = c("a_wy","bt7mx", "bt30mn", "blevCV", "bQCV", "bdLev", "bwyi", "bdQ",
                   "bReg","btCV", "sigma_wy"),  depth=2,warn=F))
```

So this means the model predicts a decrease in `DOWY` when `REG`==1, and an increase in `DOWY` with increasing `Water Index`. Interestingly, this model shows Rhats > 1, so something odd is going on here, and perhaps we should reduce the dnorm(mean back to 1 or 0). Here's the output from the **First Run** (`a ~ dnorm(0, 10)` and `a_site[site] ~ dnorm(a, sigma_site)`.


 > Changing the a ~ dnorm(0, 10) for the intercept made a big difference. Better Rhats and smaller SD across coefficients

 > Finally, changing the varying intercept to adapt not to the general intercept, but to 0 and the sigma_site appears to help.

``` 
          Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a         5.48   0.04       5.42       5.53  8453    1
a_wy[1]   0.02   0.06      -0.07       0.10  8056    1
a_wy[2]  -0.04   0.03      -0.09       0.00  8760    1
a_wy[3]  -0.03   0.03      -0.07       0.02  8895    1
a_wy[4]   0.02   0.04      -0.03       0.08  7641    1
a_wy[5]   0.01   0.04      -0.05       0.07  7785    1
a_wy[6]   0.02   0.03      -0.02       0.07  8715    1
bLEM     -0.03   0.01      -0.05      -0.02 13800    1
bt7mx     0.01   0.01      -0.01       0.03 14614    1
bt30mn   -0.01   0.01      -0.03       0.01 18636    1
bQCV      0.00   0.01      -0.01       0.02 18011    1
bdLev     0.00   0.01      -0.02       0.01 16095    1
bwyi      0.07   0.03       0.03       0.11  7893    1
bdQ       0.00   0.01      -0.02       0.01 16579    1
bReg     -0.04   0.01      -0.06      -0.01 20000    1
btCV      0.00   0.01      -0.02       0.01 20000    1
sigma_wy  0.05   0.04       0.01       0.10  4132    1
```


```{r poisson2, eval=F, echo=F}

# Model 2: scaled

mP2<- map2stan(
  alist(
    DOWY ~ dpois(mu),
    log(mu) <- a + a_site[site] + bLEM*lagEM + bt7mx*temp_7_max + bt30mn*temp_30_min + blevCV*lev_CV + bQCV*Q_CV + bdLev*deltLev + bwyi*Index + bReg*REG_id + btCV*temp_CV,
    a ~ dnorm(0,10),
    a_site[site] ~ dnorm(0, sigma_site),
    bLEM ~ dnorm(2, 10),
    c(bt7mx, bt30mn, blevCV, bQCV, bdLev, bwyi, bReg, btCV) ~ dnorm(0,10),
    sigma_site ~ dcauchy(0,1)
),
data=d2s, iter=1e4, chains=3, cores=2)

# plots and stuff
plot(mP2)
dev.off()
precis(mP2, depth=2, warn=F)
plot(precis(mP2, depth=2,warn=F))

# using BROOM & TIDY!
library(broom)
tidy(mP2@stanfit)
td_mean <- tidy(mP2@stanfit,conf.int = TRUE, rhat = T, ess = T)
td_median<-tidy(mP2@stanfit,conf.int = TRUE, rhat = T, ess = T,
     estimate.method = "median")
tds <- rbind(mutate(td_mean, method = "mean"),
               mutate(td_median, method = "median")) %>% mutate(estimate=round(estimate, 3))
head(tds)
# plot
ggplot(tds[tds$term!="dev" & tds$term!="a",], aes(estimate, term)) +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
    geom_point(aes(color = method))

# compare models
compare(mP1, mP2)
```

```{r poisson3, eval=F, echo=F}

# Model 3: scaled

mP3<- map2stan(
  alist(
    DOWY ~ dpois(mu),
    log(mu) <- a + a_wy[WY_id] + bt7mx*temp_7_max + bt30mn*temp_30_min + blevCV*lev_CV + 
      bQCV*Q_CV + bdLev*deltLev + bwyi*Index + bdQ*deltQ + bReg*REG_id + btCV*temp_CV,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bt7mx, bt30mn, blevCV, bQCV, bdLev, bwyi, bdQ, bReg, btCV) ~ dnorm(0,10),
    sigma_wy ~ dexp(1)
),
data=d2s, iter=1e4, chains=4, cores=2)



# plots and stuff
plot(mP3)
dev.off()
precis(mP3, depth=2, warn=F)
plot(precis(mP3, pars = c("a_wy","bt7mx", "bt30mn", "blevCV", "bQCV", "bdLev", "bwyi", "bdQ",
                   "bReg","btCV", "sigma_wy"), depth=2, warn=F))
abline(v=0, lty=2)

compare(mP1, mP3)

```

Looks like the model with `temp_7_max`, and `temp_30_mn` performed best, but not significantly so. The model with `ppt` and other temp values may be important. Ultimately we need to make some plots to figure this out. Also helps to create an ensemble model so that any one parameter gets "averaged" and avoids issues with trying lots of different models until you get one that "works".

I can't figure out how to extract predictions from my ensemble model for a given set of variables, or how to make a residual plot. 

#### Gaussian Regression

```{r gauss-mG4a, eval=F, echo=T}

mG4a <- map2stan(
  alist(
    DOWYprop ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] +
      bt7mx*temp_7_max + bt30mn*temp_30_min + btCV*temp_CV + # water temp
      bairmin*W_air_min + bairmax*W_air_max + # air temps
      bppt*days_no_ppt + bwyi*Index + #ppt and WY Index
      bQCV*Q_CV + bdLev*deltLev + blevavg*lev_avg + bdQ*deltQ + # flow
      bReg*REG_id, #regulation
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bt7mx, bt30mn, btCV, bairmin, bairmax, bppt, bwyi,
      bQCV, bdLev, blevavg, bdQ, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(1,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)

plot(mG4a)
dev.off()
precis(mG4a, depth=2, warn=F)
plot(precis(mG4a, depth=2,warn=F))

# using BROOM & TIDY!
library(broom)
tidy(mG4a@stanfit)
td_mean <- tidy(mG4a@stanfit,conf.int = TRUE, rhat = T, ess = T)
td_median<-tidy(mG4a@stanfit,conf.int = TRUE, rhat = T, ess = T,
     estimate.method = "median")
tds <- rbind(mutate(td_mean, method = "mean"),
               mutate(td_median, method = "median")) %>% mutate(estimate=round(estimate, 3))
head(tds)
# plot
ggplot(tds[tds$term!="dev" & tds$term!="a",], aes(estimate, term)) +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
    geom_point(aes(color = method))


```



```{r gaussian-mG4, eval=F, echo=T}

mG4 <- map2stan(
  alist(
    DOWY ~ dnorm( mu , sigma),
    mu <- a + a_site[site] + bt7mx*temp_7_max + 
      bt30mn*temp_30_min + bQCV*Q_CV + 
      bdLev*deltLev +  blevCV*lev_CV + bwyi*Index + 
      bLEM*lagEM + bdQ*deltQ + bReg*REG_id + btCV*temp_CV,
    a ~ dnorm(0 , 10) ,
    a_site[site] ~ dnorm(0, sigma_site),
    bLEM ~ dnorm(2, 10),
    bt7mx ~ dnorm( 11 , 10 ) ,
    bt30mn ~ dnorm( 7 , 10 ) ,
    bQCV ~ dnorm(5, 10),
    bdLev ~ dnorm(1, 10),
    blevCV ~ dnorm(2,10),
    bwyi ~ dnorm(6, 10),
    c(bdQ, bReg) ~ dnorm(0, 10),
    btCV ~ dnorm(8, 10),
    sigma ~ dnorm(1, 10),
    sigma_site ~ dcauchy(0,1)
  ),
  data=d2b, iter=1e4 , chains=3, cores=2
)

plot(mG4)
dev.off()
precis(mG4, depth=2, warn=F)
plot(precis(mG4, depth=1))

```

Very similar to model 5 but wider and messier.

```
             Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a          169.69  11.18     151.89     187.45  4043    1
a_site[1]   -0.82   2.24      -4.53       1.90  5408    1
a_site[2]    1.50   2.47      -1.50       5.16  3495    1
a_site[3]   -1.44   2.51      -5.12       1.78  3812    1
a_site[4]    0.89   2.23      -1.94       4.46  6520    1
bLEM        -6.54   0.78      -7.77      -5.28  5937    1
bt7mx        0.76   0.66      -0.30       1.81  5625    1
bt30mn       1.04   0.75      -0.11       2.29 10328    1
bQCV         0.06   0.20      -0.26       0.36 10106    1
bdLev        2.65   8.65     -11.23      16.31 12971    1
blevCV       1.32   0.47       0.54       2.05 10589    1
bwyi         7.73   0.47       6.99       8.50  7941    1
bdQ        -10.45   6.46     -20.38       0.28 11669    1
bReg        -7.44   3.08     -12.07      -2.97  5210    1
btCV        -0.04   0.23      -0.41       0.32 12987    1
sigma        7.77   0.60       6.83       8.69 12340    1
sigma_site   2.27   2.20       0.05       4.55  2370    1
```


```{r gaussian-mG5, eval=F, echo=T}

load("models/gaussian_full_varWY.rda" )

# scaled version
mG5 <- map2stan(
  alist(
    DOWY_s ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + bt7mx*temp_7_max + bt30mn*temp_30_min + btCV*temp_CV +
      blevCV*lev_CV + bdLev*deltLev + bQCV*Q_CV + bdQ*deltQ + bReg*REG_id +
      bppt*days_no_ppt + bH*W_humidity_avg + bWairmx*W_air_max,
    a ~ dnorm(0, 5),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bt7mx, bt30mn, blevCV, bQCV, bdLev, bdQ, bReg, btCV, 
      bWairmx, bppt, bH) ~ dnorm(0,10),
    sigma ~ dnorm(0,5),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=2e4 , chains=2, cores=2
)

png(filename = "models/mG5_precisplots.png", width = 6, height=4, units = "in", res = 72)
plot(mG5)
dev.off()
precis(mG5, depth=2, warn=F, prob=0.9)
plot(precis(mG5, depth=2, warn=F))
abline(v=0, col="red",lty=2)

compare(mG4a, mG5)
save(mG5, file = "models/gaussian_full_varWY.rda" )
```

So best model is Gaussian Model 5!

```
         Mean StdDev lower 0.89 upper 0.89 n_eff Rhat
a        -0.02   0.60      -0.95       0.92  5881    1
a_wy[1]   1.79   0.66       0.74       2.81  4529    1
a_wy[2]  -0.26   0.57      -1.16       0.61  5956    1
a_wy[3]  -0.64   0.62      -1.60       0.30  5738    1
a_wy[4]  -0.27   0.64      -1.30       0.71  6258    1
a_wy[5]  -0.83   0.63      -1.84       0.12  5435    1
a_wy[6]   0.19   0.64      -0.81       1.18  6926    1
bt7mx     0.22   0.21      -0.10       0.54  8539    1
bt30mn   -0.12   0.12      -0.30       0.05 18031    1
blevCV    0.09   0.38      -0.49       0.68 11259    1
bQCV      0.14   0.18      -0.14       0.43 12620    1
bdLev    -0.02   0.26      -0.40       0.37  8051    1
bdQ       0.04   0.21      -0.27       0.37  8372    1
bReg     -0.30   0.25      -0.69       0.07 14482    1
btCV     -0.08   0.16      -0.33       0.16 11290    1
bWairmx   0.24   0.32      -0.24       0.74 10336    1
bppt      0.15   0.18      -0.13       0.44 10993    1
bH        0.24   0.23      -0.12       0.58 10504    1
sigma     0.39   0.17       0.18       0.59  1922    1
sigma_wy  1.16   0.48       0.49       1.83  7851    1

```
### Simplify Models

Too much going on in the models above, so let's simplify and look at a few variables at a time across all sites, and see how predictions/models improve/decrease. Sticking with breeding only dates at this point, but all sites.

```{r simplifiedMods, eval=F, echo=T}

d2b <- d2 %>% filter(breed==1)
d2sb <- d2s %>% filter(breed==1)
d2sb$DOWY_s <- cusScale(d2sb$DOWY)

# Temp7max Only, DOWY scaled
mS1t <- map2stan(
  alist(
    DOWY_s ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + bt7mx*temp_7_max + bReg*REG_id,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bt7mx, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)

# delt_Lev Only, DOWY scaled
mS1dL <- map2stan(
  alist(
    DOWY_s ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + bdL*deltLev + bReg*REG_id,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bdL, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)

# Q_CV Only, DOWY scaled
mS1Qcv <- map2stan(
  alist(
    DOWY_s ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + bQCV*Q_CV + bReg*REG_id,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bQCV, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)

# delt Q Only, DOWY scaled
mS1dQ <- map2stan(
  alist(
    DOWY_s ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + bdQ*deltQ + bReg*REG_id,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bdQ, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)


# temp_CV, DOWY scaled
mS1tCV <- map2stan(
  alist(
    DOWY_s ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + btCV*temp_CV + bReg*REG_id,
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(btCV, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)

#save(mS1dL, mS1dQ, mS1Qcv, mS1t, mS1tCV, 
#     file = "models/gaussian_simple_varWY.rda")

```

```{r compare_mods}

load("models/gaussian_simple_varWY.rda")
load("models/gaussian_full_varWY.rda")

compare(mS1dL, mS1dQ, mS1Qcv, mS1t, mS1tCV, mG4a)
coeftab(mS1dL, mS1dQ, mS1Qcv, mS1t, mS1tCV, mG5)

logistic(coef(mG5))
postcheck(mG5)
postcheck(mG4a)
rethinking::coeftab_plot(mS1tCV)

# sample simple:
postQcv <- extract.samples(mS1Qcv)
simQcv <- rnorm(8000, (postQcv$a + postQcv$bQCV), postQcv$sigma)
simQcv <- rnorm(8000, (postQcv$a), postQcv$sigma)

plot( NULL , xlim=c(-3.5,4), ylim=c(0,1),
    xlab="log-odds breed" , ylab="Density" )

for ( i in 1:100 )
    curve( dnorm(x,(postQcv$a[i]+postQcv$bQCV[i]*mean(d2sb$Q_CV)),postQcv$sigma_wy[i]) , 
           add=TRUE ,col=col.alpha("black",0.2) )

dens( logistic(simQcv) , xlab="probability Qcv influence DOWY" )


```

It looks like the best model is `tCV`, `t7mx`, `Qcv`, `dL`, then `dQ`. But not much differentiating these, max difference in WAIC is `2.7` between all models, and weight is fairly evenly distributed.

```
      WAIC pWAIC dWAIC weight    SE  dSE
mS1tCV 31.4   8.3   0.0   0.35 10.70   NA
mS1t   32.3   8.0   0.9   0.22 10.26 2.11
mS1Qcv 32.4   8.1   1.0   0.21 10.18 5.68
mS1dL  33.2   8.3   1.8   0.14 11.20 5.35
mS1dQ  34.1   8.1   2.7   0.09 10.99 3.27
```

```{r betabinomial}
# earliest possible breeding obs in Sierras is NF Feather (near Apr 1), use days since Apr 1 each year as the difference in RATE?)
d2b$dowy_apr1 <- d2b$DOWY-182
data(UCBadmit)

mBB1 <- map2stan(
    alist(
        dowy_apr1 ~ dbetabinom(DOWY,pbar,theta),
        logit(pbar) <- a + btCV*temp_CV,
        a ~ dnorm(0,5),
        btCV ~ dnorm(0,5),
        theta ~ dexp(1)
    ),
    data=d2b,
    constraints=list(theta="lower=0"),
    start=list(theta=2),
    iter=4000 , warmup=1000 , chains=2 , cores=2 )

plot(mBB1)
dev.off()
precis(mBB1)

# so implied average probability of breeding across all sites/years is: 
postBB <- extract.samples(mBB1)
quantile( logistic(postBB$a) , c(0.025,0.5,0.975) )

# plot
# draw posterior mean beta distribution
curve( dbeta2(x,mean(logistic(postBB$a + postBB$btCV)),mean(postBB$theta)) , from=0 , to=1 ,
    ylab="Density" , xlab="probability breed", ylim=c(0,5) , lwd=2 )
# draw 100 beta distributions sampled from posterior
for ( i in 1:100 ) {
    p <- logistic( postBB$a[i] + postBB$btCV[i])
    theta <- postBB$theta[i]
    curve( dbeta2(x,p,theta) , add=TRUE , col=col.alpha("black",0.2) )
}

postcheck(mBB1)
dev.off()
```



```{r broom_tables_simple}

# using BROOM & TIDY!
library(broom)

td_mean <- tidy(mS1tCV@stanfit,conf.int = TRUE, rhat = T, ess = T)
td_median<-tidy(mS1tCV@stanfit,conf.int = TRUE, rhat = T, ess = T,
     estimate.method = "median")

tds <- rbind(mutate(td_mean, method = "mean"),
               mutate(td_median, method = "median")) %>% mutate(estimate=round(estimate, 3))
head(tds)

# plot
ggplot(tds[tds$term!="dev" & tds$term!="a",], aes(estimate, term)) +
  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
  geom_point(aes(fill = method), pch=21, alpha=0.6, size=4)+
  geom_vline(xintercept=0, lty=2, col="maroon")
  
```

### PLOTS: Simple Counterfactual

```{r ctrfactual_t7mx}

post1_t7<-extract.samples(mS1t)

d2sb$site <- d2b$site # change back to site name

# test plot
plot( DOWY_s ~ temp_7_max , data=d2sb, pch=ifelse(d2sb$WY_id==1, 16, 21), col=ifelse(d2sb$REG_id==0, rangi2, "red"), cex=2.5)

# the sequence to link model to
temp_7_max.seq <- seq( from=-1.5 , to=3 , length.out = 30)

# use link to compute mu for each sample from posterior
# and for each temp in temp.seq
mu1_U <- link( mS1t, 
            data=data.frame(temp_7_max=temp_7_max.seq,
                            REG_id=0, WY_id=6))
mu1_R <- link( mS1t, 
            data=data.frame(temp_7_max=temp_7_max.seq,
                            REG_id=1, WY_id=6))

# summarize MU
mu1U.mean <- apply( mu1_U , 2 , mean )
mu1R.mean <- apply( mu1_R , 2 , mean )

# mean DOWY for unreg:
mean(unScale(mu1U.mean, d2sb$DOWY)) # 268 (2011),  230 (2012),  221 (2013), 224 (2014), 217 (2015), 242 (2016)
Ut7mx<-tibble("year"=c(2011:2016), "meanDOWY"=c(268,230,221,224,217, 242))

# mean DOWY for Reg:
mean(unScale(mu1R.mean, d2sb$DOWY)) # 258 (2011), 220, (2012), 211 (2013), 214 (2014), 207 (2015), 232 (2016)
Rt7mx<-tibble("year"=c(2011:2016), "meanDOWY"=c(258,220,211,214,207, 232))
points(Rt7mx, col="red", pch=16)

# So a 10 day mean difference between regulated and unregulated river

# summarize the distribution of mu
mu1U.HPDI89 <- apply( mu1_U , 2 , HPDI , prob=0.89 )
mu1R.HPDI89 <- apply( mu1_R , 2 , HPDI , prob=0.89 )
mu1U.HPDI97 <- apply( mu1_U , 2 , HPDI , prob=0.97 )
mu1R.HPDI97 <- apply( mu1_R , 2 , HPDI , prob=0.97 )
mu1U.HPDI67 <- apply( mu1_U , 2 , HPDI , prob=0.67 )
mu1R.HPDI67 <- apply( mu1_R , 2 , HPDI , prob=0.67 )

# make ggplot HPDIs
U_ci <-tibble(CI_L=mu1U.HPDI97[1,], CI_U=mu1U.HPDI97[2,]) %>% cbind(., temp_7_max.seq)
R_ci <-tibble(CI_L=mu1R.HPDI97[1,], CI_U=mu1R.HPDI97[2,]) %>% cbind(., temp_7_max.seq)


# use type="n" to hide raw data
library(viridis)
viridis(6, option = "B")
# "#000004FF" "#3B0F70FF" "#8C2981FF" "#DE4968FF" "#FE9F6DFF" "#FCFDBFFF"
plot( DOWY_s ~ temp_7_max , data=d2sb, pch=ifelse(d2sb$WY_id==1, 21, 23), bg=ifelse(d2sb$REG_id==0, "#000004FF", "#932667FF"), col="gray60", ylim=c(-2,2.5), xlim=c(-1.5,3), cex=2.5)

#mean(d2$DOWY) # 213.86
#sd(d2$DOWY) # 22.18837

# lookup the scaled equiv
y_labs<-tibble("DOWY"=round(unScale(c(-2, -1.5, -1, -0.5, 0, 0.5, 1.0, 1.5, 2, 2.5), d2b$DOWY), 0)) %>% 
  inner_join(., dowy_all, by = "DOWY") %>% select(monDay) %>% as.data.frame()

(x_labs<-tibble("temp_7_max"=round(unScale(c(seq(-1.5, 3, 0.5)), d2b$temp_7_max), 0)) %>% as.data.frame())

# ggplot version
ggplot() + 
  #ylim(c(-2,2.5)) + 
  #xlim(c(-1.5,3)) + 
  theme_light() + 
  geom_ribbon(data=U_ci, aes(x=temp_7_max.seq , ymin=CI_L, ymax=CI_U), fill = "#000004FF", alpha=0.5)+
  geom_ribbon(data=R_ci, aes(x=temp_7_max.seq , ymin=CI_L, ymax=CI_U), fill = "#932667FF", alpha=0.5)+
  scale_y_continuous("Date", breaks=c(seq(-2,2.5,0.5)),
                   labels= y_labs[,1])+
  scale_x_continuous("Water Temp (7-day Max)", breaks=c(seq(-1.5, 3, 0.5)),
                   labels= x_labs[,1])+
  geom_line(aes(x=temp_7_max.seq , y=mu1U.mean), lwd=2, color="#000004FF" ) + #unreg
  geom_line(aes(x=temp_7_max.seq , y=mu1R.mean), lwd=2, color="#932667FF" ) + #reg
  geom_point(data=d2sb, aes(x=temp_7_max, y=DOWY_s), fill=ifelse(d2sb$REG_id==0, "#000004FF", "#932667FF"), pch=ifelse(d2sb$WY_id==1, 21, 23), size=5, show.legend = F)

ggsave(filename = "models/G_simp_ms1tmx_vs_mean.png", width=9, height=6)

# add axes
# aty <- seq(183, 289, by = 15)
# labelsY<-dowys$mon[8:15]
# axis(side=2, at = aty, labels=labelsY, las=1)

# loop over samples and plot each mu4 value
# for ( i in 1:50 )
#     lines( temp_7_max.seq , mu1_U[i,],
#            col=col.alpha(rangi2,0.2) )

lines( temp_7_max.seq , mu1U.mean, lwd=2, col="#000004FF" ) #unreg
#text(1.9, 0.3, labels = "Unreg (u=224)", col="darkblue")
lines( temp_7_max.seq , mu1R.mean, lwd=2, col="#DE4968FF", lty=2 ) #reg
#text(-0.7, -1.3, labels = "Reg (u=214)", col="maroon")

# 97% prob intervals
shade( mu1U.HPDI97 , temp_7_max.seq,
      col = col.alpha("#000004FF",alpha=0.2))
shade( mu1R.HPDI97 , temp_7_max.seq,
      col = col.alpha("#DE4968FF",alpha=0.2))

# identify some points
identify( x=d2sb$temp_7_max , y=d2sb$DOWY_s , 
          labels=paste0(d2sb$site,"-",d2sb$WY_id), cex=1.5 )


```


```{r counterfactual_QCV}

# extract posterior estimates
post1_QCV<-extract.samples(mS1Qcv)

# the sequence to link model to
Q_CV.seq <- seq(from=-1.5 , to=4 , length.out = 36)

# use link to compute mu for each sample from posterior
# and for each temp in temp.seq
mu1_U <- link( mS1Qcv, 
            data=data.frame(Q_CV=Q_CV.seq,
                            REG_id=0, WY_id=3))
mu1_R <- link( mS1Qcv, 
            data=data.frame(Q_CV=Q_CV.seq,
                            REG_id=1, WY_id=3))

# summarize MU
mu1U.mean <- apply( mu1_U , 2 , mean )
mu1R.mean <- apply( mu1_R , 2 , mean )

# mean DOWY for unreg:
mean(unScale(mu1U.mean, d2sb$DOWY)) # 228
# mean DOWY for Reg:
mean(unScale(mu1R.mean, d2sb$DOWY)) # 218 (grand mean)

# So a 10 day mean difference between regulated and unregulated river

# summarize the distribution of mu
mu1U.HPDI89 <- apply( mu1_U , 2 , HPDI , prob=0.89 )
mu1R.HPDI89 <- apply( mu1_R , 2 , HPDI , prob=0.89 )
mu1U.HPDI97 <- apply( mu1_U , 2 , HPDI , prob=0.97 )
mu1R.HPDI97 <- apply( mu1_R , 2 , HPDI , prob=0.97 )
mu1U.HPDI67 <- apply( mu1_U , 2 , HPDI , prob=0.67 )
mu1R.HPDI67 <- apply( mu1_R , 2 , HPDI , prob=0.67 )

# use type="n" to hide raw data
plot( DOWY_s ~ Q_CV , data=d2sb, pch=ifelse(d2sb$WY_id==1, 16, 21), col=ifelse(d2sb$REG_id==0, rangi2, "red"), ylim=c(-1.5, 2.5), xlim=c(-1.5,4.25))

lines( Q_CV.seq , mu1U.mean, lwd=2, col="darkblue" ) #unreg
text(3, 0.6, labels = "Unreg (u=237)", col="darkblue")
lines( Q_CV.seq , mu1R.mean, lwd=2, col="maroon", lty=2 ) #reg
text(2.7, -0.7, labels = "Reg (u=227)", col="maroon")

# 97% prob intervals
shade( mu1U.HPDI97 , Q_CV.seq,
      col = col.alpha("darkblue",alpha=0.2))
shade( mu1R.HPDI97 , Q_CV.seq,
      col = col.alpha("maroon",alpha=0.2))

# identify some points
identify( x=d2sb$Q_CV , y=d2sb$DOWY_s , 
          labels=paste0(d2sb$site,"-",d2sb$WY_id), cex=0.8 )

```

#### PLOTS: Obs vs. Predicted

```{r simdata_obs_pred}

mu1s <- link( mS1s )

# summarize samples across cases
mu.mean <- apply( mu1s , 2 , mean )
mu.PI <- apply( mu1s , 2 , PI )

# simulate observations, uses original data
temp.sim <- sim( mS1s , n=1e4 )
temp.PI <- apply( temp.sim , 2 , PI )

# now plot
plot( mu.mean ~ d2sb$DOWY_s , pch=16,
      col=ifelse(d2sb$REG_id==0, rangi2, "red"), 
      ylim=range(mu.PI) ,
      xlab="Observed Date of Spawning" , 
      ylab="Predicted Date of Spawning")
abline( a=0 , b=1 , lty=2 )

for ( i in 1:nrow(d2sb) )
    lines( rep(d2sb$DOWY_s[i],2) , c(mu.PI[1,i],mu.PI[2,i]) ,
        col=ifelse(d2sb$REG_id[i]==0, rangi2, "red") )

identify( x=d2sb$DOWY_s , y=mu.mean , 
          labels=paste0(d2sb$site,"-",d2sb$WY_id), cex=0.8 )

```

Predicted spawning dates agains observed, with 89% confidence intervals of the average prediction, dashed line is perfect prediction.


#### PLOTS: Prediction Error

```{r plot Prediction Error}
# compute residuals
dowy.resid <- d2sb$DOWY_s - mu.mean

# get ordering by DOWY rate
o <- order(dowy.resid)

# make the plot
dotchart( dowy.resid[o] ,
          labels=paste(d2sb$site[o],"-",d2sb$WY_id[o]) ,
          xlim=c(-2,2) , cex=0.6 )
abline( v=0 , col=col.alpha("black",0.2) )

for ( i in 1:nrow(d2sb) ) {
    j <- o[i] # which site in order
    lines( d2sb$DOWY_s[j]-c(temp.PI[1,j],temp.PI[2,j]) , rep(i,2) )
    points( d2sb$DOWY_s[j]-c(temp.PI[1,j],temp.PI[2,j]) ,
            rep(i,2),
            pch=3 , cex=0.6 , col=rangi2 )
}

```

Average prediction error for each site, with 89% interval of the mean (black line) and 89% prediction interval (blue +)

```{r residual plot}

# compute expected value at MAP,for each obs
mu <- coef(mS1s)['a']+coef(mS1s)['a_wy[4]'] + coef(mS1s)['bt7mx']*d2sb$temp_7_max

mu <- coef(mS1s)['a'] + coef(mS1s)['bdL']*d2sb$deltLev

# compute residual for each point
m.resid <- d2sb$temp_7_max - mu
m.resid <- d2sb$deltLev - mu

# if resid positive it means obs rate is in excess of what we would expect
plot( DOWY_s ~ deltLev , d2sb , 
      col=ifelse(d2sb$REG_id==0, rangi2, "red"), pch=16,
      xlim=c(-2,3.5), ylim=c(-2,3))
abline( a=coef(mS1s)["a"], b=coef(mS1s)['bdL'])

# temp
plot( DOWY_s ~ temp_7_max , d2sb , 
      col=ifelse(d2sb$REG_id==0, rangi2, "red"), pch=16,
      xlim=c(-2,3.5), ylim=c(-2,3))
abline( a=(coef(mS1s)["a_wy[4]"]+coef(mS1s)["a"]),
        b=coef(mS1s)['bt7mx'])

# loop over points
for ( i in 1:length(m.resid) ) {
    #x <- d2sb$temp_7_max[i] # x location of line segment
    x <- d2sb$deltLev[i]
    y <- d2sb$DOWY_s[i] # observed endpoint of line segment
    # draw the line segment
    lines( c(x,x) , c(mu[i],y) , lwd=0.5 , col=col.alpha("black",0.7) )
}

identify( x=d2sb$temp_7_max , y=d2sb$DOWY_s , 
          labels=paste0(d2sb$site,"-",d2sb$WY_id), cex=0.8 )

identify( x=d2sb$deltLev , y=d2sb$DOWY_s , 
          labels=paste0(d2sb$site,"-",d2sb$WY_id), cex=0.8 )


```

Points above the line have higher (later) than expected breeding times, vs. below they have earlier than expected breeding times

### Interaction Models
```{r interaction models}

# Interaction w WY
mS2 <- map2stan(
  alist(
    DOWY ~ dnorm( mu , sigma),
    mu <- a + a_wy[WY_id] + bt7mx*temp_7_max + bReg*REG_id +
      bt7mxWY*(temp_7_max * Index),
    a ~ dnorm(0, 10),
    a_wy[WY_id] ~ dnorm(0, sigma_wy),
    c(bt7mx, bt7mxWY, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_wy ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)

# Interation w WY, intercept by SITE
mS3 <- map2stan(
  alist(
    DOWY ~ dnorm( mu , sigma),
    mu <- a + a_site[site] + bt7mx*temp_7_max + bReg*REG_id +
      bt7mxWY*(temp_7_max * Index),
    a ~ dnorm(0, 10),
    a_site[site] ~ dnorm(0, sigma_site),
    c(bB, bt7mx, bt7mxWY, bReg) ~ dnorm(0,10),
    sigma ~ dnorm(0,10),
    sigma_site ~ dcauchy(0,1)
  ),
  data=d2sb, iter=1e4 , chains=3, cores=2
)


plot(mS2)
dev.off()
precis(mS2, depth=2, warn=F)
precis(mS3, depth=2, warn=F)
plot(precis(mS2, depth=2))
plot(precis(mS3, depth=2))

compare(mS2, mS3)

# using BROOM & TIDY!
library(broom)
tidy(mP2@stanfit)
td_mean <- tidy(mP2@stanfit,conf.int = TRUE, rhat = T, ess = T)
td_median<-tidy(mP2@stanfit,conf.int = TRUE, rhat = T, ess = T,
     estimate.method = "median")
tds <- rbind(mutate(td_mean, method = "mean"),
               mutate(td_median, method = "median")) %>% mutate(estimate=round(estimate, 3))
head(tds)
# plot
ggplot(tds[tds$term!="dev" & tds$term!="a",], aes(estimate, term)) +
    geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +
    geom_point(aes(color = method))
```


#### OLD Plots

```{r plots_mG5_t7max}
post4<-extract.samples(mG4)
post5<-extract.samples(mG5)

plot( DOWY ~ temp_7_max , data=d2, pch=ifelse(d2$lagEM==1, 16, 21), col=ifelse(d2$REG_id==0, rangi2, "red"))

temp_7_max.seq <- seq( from=6 , to=18 , by=0.5)

# use link to compute mu for each sample from posterior
# and for each temp in temp.seq
mu4 <- link( mG4, 
            data=data.frame(temp_7_max=temp_7_max.seq,lagEM=1, 
                            temp_30_min=8, lev_CV=0, site=1,
                            Q_CV=0, deltLev=0, Index=6,
                            deltQ=0, REG_id=0, temp_CV=0))
mu5 <- link( mG5, 
            data=data.frame(temp_7_max=temp_7_max.seq,lagEM=1, 
                            temp_30_min=8, lev_CV=0, site=1,
                            Q_CV=0, deltLev=0, Index=6,
                            deltQ=0, REG_id=0, temp_CV=0))

# use type="n" to hide raw data
plot( DOWY ~ temp_7_max , data=d2, pch=ifelse(d2$lagEM==1, 16, 21), col=ifelse(d2$WY==2011, rangi2, "red"), yaxt='n', ylab="", xlim=c(6,18))

# add axes
aty <- seq(183, 289, by = 15)
labelsY<-dowys$mon[8:15]
#axis(side=2, at = aty, las=1)
axis(side=2, at = aty, labels=labelsY, las=1)

# loop over samples and plot each mu4 value
# for ( i in 1:50 )
#     lines( temp_7_max.seq , mu4[i,] , pch=21 , col="gray", bg=col.alpha(rangi2,0.2) )


# summarize the distribution of mu4
mu4.mean <- apply( mu4 , 2 , mean )
mu4.HPDI89 <- apply( mu4 , 2 , HPDI , prob=0.89 )
mu4.HPDI97 <- apply( mu4 , 2 , HPDI , prob=0.97 )
mu4.HPDI67 <- apply( mu4 , 2 , HPDI , prob=0.67 )

lines( temp_7_max.seq , mu4.mean )
shade( mu4.HPDI97 , temp_7_max.seq)
shade( mu4.HPDI89 , temp_7_max.seq)
shade( mu4.HPDI67 , temp_7_max.seq)


# identify some points
identify( x=d2$temp_7_max , y=d2$DOWY , 
          labels=paste0(d1$site,"-",d2$WY), cex=0.8 )
```



```{r plots_mG4_levDelt}
deltLev.seq <- seq( from=-5 , to=2.3 , length.out = 40)
#unScale(deltLev.seq, d2$deltLev)

# use link to compute mu for each sample from posterior
# and for each temp in temp.seq
mu4 <- link( mG4, 
            data=data.frame(temp_7_max=10, lagEM=1, 
                            temp_30_min=8, lev_CV=0, site=4,
                            Q_CV=0, Index=6,
                            deltQ=0, REG_id=0, temp_CV=0,
                            deltLev=deltLev.seq))

# use type="n" to hide raw data
plot( DOWY ~ deltLev , data=d2, pch=ifelse(d2$lagEM==1, 16, NA), col=ifelse(d2$WY==2011, rangi2, "red"))

# loop over samples and plot each mu value
for ( i in 1:40 )
    lines( deltLev.seq , mu4[i,],
           col=col.alpha(rangi2,0.2) )# 10C

# summarize the distribution of mu
mu4.mean <- apply( mu4 , 2 , mean )
mu4.HPDI89 <- apply( mu4 , 2 , HPDI , prob=0.89 )
mu4.HPDI97 <- apply( mu4 , 2 , HPDI , prob=0.97 )
mu4.HPDI67 <- apply( mu4 , 2 , HPDI , prob=0.67 )

plot( DOWY ~ deltLev , data=d2, pch=ifelse(d2$lagEM==1, 16, NA), col=ifelse(d2$WY==2011, rangi2, "red"))

lines( deltLev.seq , mu4.mean )
shade( mu4.HPDI97 , deltLev.seq, col = col.alpha("purple3", 0.2) )
shade( mu4.HPDI89 , deltLev.seq)
shade( mu4.HPDI67 , deltLev.seq, col=col.alpha("gray10", 0.2))

# identify some points
identify( x=d2$deltLev , y=d2$DOWY , 
          labels=paste0(d2$site,"-",d2$WY), cex=0.8 )

```

Maybe try an obs vs. modeled plot but use the regulated data...see how far off it is from the observed.


#### Notes from Myfanwy 2017-05-24

Use DOWY, not binomial, sites are coin flips. What triggers breeding, but doesn't help answer more important factors of what drives breeding? Want response to be as continuous as possible. 

**DOWY as response**
gammaPoisson, has individual "rate", rate should be affected in same way by different predictor variables. Different sites as nested variables, rate varies by site. 

**Survival Analysis**

How long does an event take to occur, (i.e., time to breeding)? Freezing over in a lake as example. Continuous segmented regression, (tobit regression) + probit model with time series with latent variable.

**MultiState Model**

predicting frequencies that will be occupied on radio, occupied vs. not.

Hidden Markov model, model conditions leading up to observable state (hidden state). Hidden markov poisson, describe variability that characterizes this latent state that you don't observe, but you know it drives the latent state.

Eric Van Cleaves (multistate modeler, spatial component)

Input matrix: 365 x 30 variables
Each row is day of year
Whether a breeding event occurred on that day as binomial




```{r gamPoisson}

d1f <- d1 %>% filter(breed==1) # NFY and NFA breeding only

#change delts to positive:
d1f$deltQexp <- exp(d1f$deltQ)
d1f$deltLevexp <- exp(d1f$deltLev)
d1f$log_DOWY <- log(d1f$DOWY)

mgP1 <- map2stan(
  alist(
    log_DOWY ~ dgampois(mu, scale),
    log(mu) <- a + bt7mx*temp_7_max + bwyi*Index + bdeltLev * deltLevexp +
      bWairmx*W_air_30_max + blevCV*lev_CV,
    a ~ dnorm(0, 10),
    c(bt7mx, bwyi, bdeltLev, bWairmx, blevCV) ~ dnorm(0,1),
    scale ~ dcauchy(0,1)
  ),
  data=d1f,
  constraints=list(scale="lower=0"),
  start=list(scale=1),
  warmup=1000, iter=8000, chains=2, cores=2)

precis(mgP1)
plot(mgP1)

postcheck(mgP1)
#blue points show the empirical proportion on each row of the data
#open circles are the posterior mean p, with 89% percentile interval
# + symbols mark the 89% interval of predicted counts for breeding. Lots of dispersion expected here.


postgP1 <- extract.samples(mgP1)
quantile( logistic(postgP1$a) , c(0.025,0.5,0.975) )

# draw posterior mean dgam distribution
curve( dgampois(x,mean(logistic(postmgP1$a)),mean(postmgP1$scale)) , from=0 , to=1 ,
    ylab="Density" , xlab="probability", ylim=c(0,3) , lwd=2 )
# draw 100 beta distributions sampled from posterior
for ( i in 1:100 ) {
    p <- logistic( postmgP1$a[i] )
    scale <- postmgP1$scale[i]
    curve( dgampois(x,p,scale) , add=TRUE , col=col.alpha("black",0.2) )
}


```





```{r}
# show density of log-odds
coef(mG4)

# make some plots
plot(DOWY ~ Q_CV, data=d2, pch=ifelse(d2$lagEM==1, 16, 21), col=ifelse(d2$WY==2011, rangi2, "red"))

abline(a=coef(mG4)["a"], b=coef(mG4)["bQCV"]*d2$Q_CV,
       col="black", lty=2)
abline(a=coef(mG4)["a"], b=coef(mG4)["bQCV"]*d2$Q_CV,
       col="black", lty=2)


# vars:
#a_lag[lagEM] + bt7mx*temp_7_max + bt30mn*temp_30_min + bQCV*Q_CV + bdLev*deltLevEX

# counterfactual
d.pred <- list(
    lagEM = rep(1,30),
    #DOWY = seq(150,300, length.out = 50),
    temp_7_max = seq(6,16, length.out = 30)
    #temp_30_min = seq(4,16, length.out = 30),
    #Q_CV = seq(0,40, length.out = 30),
    #deltLevEX = seq(0.7, 1.2, length.out = 30)
)

#a_lag_sims <- rnorm(20000,,post1$sigma_lag)
#a_lag_sims <- matrix(a_lag_sims,2000,10)
link.mP1 <- link( mP2 , n=2000 , data=d.pred)

# plot raw data
plot(d1$DOWY~d1$temp_7_max, pch=ifelse(d1$lagEM==1, 16, 21),
     col=ifelse(d1$WY==1, rangi2, "red"),
     #xlim=c(0,300), ylim=c(0,30),
     xlab="Temp 7 max" , ylab="DOWY")

# plot posterior median
mu.median <- apply( link.mP1 , 2 , median )
lines(d.pred$temp_7_max, mu.median)

# plot 97%, 89%, and 67% intervals (all prime numbers)
mu.PI <- apply( link.mP1 , 2 , PI , prob=0.97 )
shade(mu.PI, d.pred$temp_7_max )
mu.PI <- apply( link.mP1 , 2 , PI , prob=0.89 )
shade(mu.PI, d.pred$temp_7_max )
mu.PI <- apply( link.mP1 , 2 , PI , prob=0.67 )
shade(mu.PI, d.pred$temp_7_max )

```

```{r jacobgampoisson}

#########
# from Jacob 05/17/17
mz43 <- map2stan(
  alist(
    Zoops ~ dgampois(mu,scale),
    log(mu) <- a_site[site_id] + bt*temp + bno * NO3.N + bp * PO4.P + bd * DOC + bj * JulianDay,
    a_site[site_id] ~ dcauchy(10,sigma_site),
    sigma_site ~ dcauchy(1,1),
    c(bt,bno,bp,bd,bj) ~ dnorm(0,1),
    scale ~ dcauchy(0,2)
  ),
  data=zb,
  constraints=list(scale="lower=0"),
  start=list(scale=2)
)

```

