---
title: "Oviposition Logistic"
author: "Ryan Peek"
date: "Updated: `r format(Sys.Date())`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("../"))
```

## Try Model, Model Fail

Let's try to assess whether water temperature, air temperature, flow (daily/weekly recession rates & discharge) play a role in predicting when breeding (oviposition occurs). To build this model we need to block by river, and by water year, as these are independent (though water year isn't *fixed*, it is fixed across all sites per each year).

First let's take a look at some test data. Using a subset (say just pull the **NF American**), can we build a model that gets at whether there is predictability in these variables despite shifts in timing and magnitude from year to year?

### Get the Data & Tidy

```{r get and tidy data}

library(tidyverse)
library(lubridate)

load("data/master_dat_2011-2016.rda") 
load("data/flow_dv_cfs_2011_6sites.rda") # updated and merged flows:

df <- master_df
# df <- master_df %>% filter(site!="MFY")
# df <- master_df %>% filter(month(date)>2 & month(date)<8 & !site=="MFA")
# df <- master_df %>% filter(month(date)>3 & month(date)<8 & site=="NFA" | site=="NFY")


# 2 NAs in dataset from 30 day avgs
df[!is.na(df$missData) & df$site=="RUB" & df$date==ymd("2011-05-26"),]$temp_30_max<-10.99
df[!is.na(df$missData) & df$site=="RUB" & df$date==ymd("2011-05-26"),]$temp_30_min<-9.7

rabo <- df[!is.na(df$missData),] %>% arrange(date)

# add unique rownames based on siteID
rabo <- rabo %>%
  mutate("siteID" = paste0(site, "-", row.names(.))) %>% 
  column_to_rownames(var = "siteID")  %>% as.data.frame
#names(rabo)
#rownames(rabo)

# create column with date for 15 and 30 day lags (for binomial logistic)
rabo_lag <- rabo %>% 
  mutate(d07 = date - 7, 
         d14 = date - 14,
         d21 = date - 21,
         d30 = date - 30) %>% 
  select(site, date, WY, d07:d30)

# rejoin / filter form orig dataset

rabo_bin7<- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d07")) %>% select(-c(date.y, d14:d30)) %>% mutate(lagEM="d07")
rabo_bin14 <- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d14")) %>% select(-c(date.y, d07:d30)) %>% mutate(lagEM="d14")
rabo_bin21 <- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d21")) %>% select(-c(date.y, d07:d30)) %>% mutate(lagEM="d21")
rabo_bin30 <- inner_join(df, rabo_lag, by=c("site"="site", "WY"="WY", "date"="d30")) %>% select(-c(date.y, d07:d21)) %>% mutate(lagEM="d30")

# bind up for one dataset of lags, add 1/0 col for breeding
rabo_LAGS<-rbind(rabo_bin7, rabo_bin14, rabo_bin21, rabo_bin30) %>% 
  mutate(breed = 0)
rm(list=ls(pattern = "rabo_bin*"))

# add breed col to orig dataset:
rabo <- rabo %>% mutate(breed = 1, lagEM="d00")

# make final logistic dataset, add 1/0 column
rabo_logist <- rbind(rabo_LAGS, rabo)

```


## Rethinking Models

Prepare data by removing much of the information that will be unused, and converting and scaling the data as needed.


```{r prepData, eval=T, echo=T}
library(rethinking)

# Filter down data, use only unreg sites
d1 <- dplyr::select(rabo_logist, site, breed, lagEM, DOWY, everything(), -date, -EM_per_km, -REG,
                     -(obs_strt:apr_jul), -len_km, -starts_with("CDEC"), 
                     -station, -WYsum, -lev_7_avg, -Q_cfs, 
                     -Q_min, -Q_7_cfs) %>% 
    filter(!site=="MFA") %>% 
#    filter(site=="NFY" | site=="NFA") %>%
  as.data.frame

# RM NAs, add id and site to rownames:
d1 <- d1 %>% filter(!is.na(temp_30_min))
d1_rownames<- paste0(d1$site, "-",d1$WY, "-", seq(1:nrow(d1)))
d1$site <- as.factor(d1$site)
d1$lagEM <- as.integer(as.factor(d1$lagEM))

# custom scale function
cusScale <- function(x){
  (x - mean(x))/sd(x)
}

# scale all data and rebind site and breed info:
d1s <- d1
d1s[,c(4:31)] <- apply(d1[,c(4:31)], 2, cusScale)
d1s$site <- as.integer(d1s$site)

d1s <- d1s %>% rename(river=site, wyind=Index)


```

Intercept only model

```{r dbinom_intercept, eval=F, echo=T}

# based on m10.1

# an intercept only model of breeding
m1.1 <- map(
    alist(
        breed ~ dbinom( 1 , p ) ,
        logit(p) <- a ,
        a ~ dnorm(0,10)
),
    data=d1s )
precis(m1.1)
logistic(c(-1.56, -1.19, -0.82)) # about a 23% chance of breeding based on these data
```

#### Start Simple: temp only no varying intercept

```{r temp_only}

# just temperature
m1.1a <- map2stan(
    alist(
        breed ~ dbinom( 1 , p ) ,
        logit(p) <- a + bt7mx*temp_7_max + bt7mn*temp_7_min + bt30mx*temp_30_max + bt30mn*temp_30_min,
        a ~ dnorm(0,1),
        c(bt7mx, bt7mn, bt30mx, bt30mn) ~ dnorm(0,1)
    ), data=d1s, warmup=1000, iter=5000, chains=4, cores=2)

precis(m1.1a, corr = T)
logistic(c(-1.41, -0.28, .64, 0.57, 0.30)) # about a 23% chance of breeding based on these data
plot(precis(m1.1a))

# show density of probabilities
post <- extract.samples(m1.1a)
dens( post$bt7mx , xlab="Prob", ylim=c(0,1.5))
dens( post$bt30mx, col=rangi2 , lwd=2 , add=TRUE )
dens( post$bt7mn , col="red", add=TRUE)
dens( post$bt30mn, col="maroon", lwd=2 , add=TRUE )
text( 0.4 , 1 , "30dmx" , col=rangi2 )
text( 0.85 , 1 , "7dmx" )


# now just 7day
m1.1b <- map2stan(
    alist(
        breed ~ dbinom( 1 , p ) ,
        logit(p) <- a + bt7mx*temp_7_max + bt7av*temp_7_avg + btCV*temp_CV ,
        a ~ dnorm(0,1),
        c(bt7mx, bt7av, btCV) ~ dnorm(0,1)
    ), data=d1s, warmup=1000, iter=5000, chains=4, cores=2)

plot(m1.1b)
precis(m1.1b)

# show density of probabilities
post1b <- extract.samples(m1.1b)
dens( post1b$bt7mx , ylim=c(0,1.5), xlim=c(-1.4,3.4))
dens( post1b$bt7av, col=rangi2 , lwd=2 , add=TRUE )
dens( post1b$btCV , col="red", add=TRUE)

plot(precis(m1.1b))

# now add interaction
m1.1c <- map2stan(
    alist(
        breed ~ dbinom( 1 , p ) ,
        logit(p) <- a + bt7mx*temp_7_max + btCV*temp_CV + bppt*days_no_ppt ,
        a ~ dnorm(0,1),
        c(bt7mx, btCV, bppt) ~ dnorm(0,1)
    ), data=d1s, warmup=1000, iter=5000, chains=4, cores=2)

plot(m1.1c)
precis(m1.1c)
plot(precis(m1.1c))

# show density of probabilities
post1c <- extract.samples(m1.1c)
dens( post1c$bt7mx , ylim=c(0,1.7), xlim=c(-1,2.4))
dens( post1c$btCV, col=rangi2 , lwd=2 , add=TRUE )
dens( post1c$bppt , col="red", add=TRUE)


compare(m1.1a, m1.1b, m1.1c)
```

### Varying Intercept

```{r temp varying intercept}

names(d1s)
s(d1s)
d1sA <- d1s %>% select(breed, river, temp_7_max, wyind)
d1sA$breed<- as.integer(d1sA$breed)
str(d1sA)

# now just 7day
m2.1a <- map2stan(
    alist(
        breed ~ dbinom( 1 , theta ) ,
        logit(theta) <- a + a_river[river] + bt7mx*temp_7_max + bwyind*wyind,
        a_river[river] ~ dnorm( 0, sigma_river ),
        c(a,bt7mx,bwyind) ~ dnorm(0,10),
        sigma_river ~ dcauchy(0,1)
    ), 
    data=d1sA, warmup=1000, iter=5000, chains=4, cores=2)

# view output
plot(m2.1a)
precis(m2.1a)
plot(precis(m2.1a, depth=2))

# show density of probabilities
post2a <- extract.samples(m2.1a)
dens( post2a$bt7mx , ylim=c(0,1.7), xlim=c(-1,2.4))
dens( post2a$bwyind, col=rangi2 , lwd=2 , add=TRUE )
dens( post2a$sigma_river , col="red", add=TRUE)
```


Now add other variables into the model, with partial pooling (but no interactions):

```{r dbinom_fixed, eval=F, echo=T}

d1.2 <- select(d1s, breed, site, lagEM, temp_7_max, Index, deltLev, Q_CV, deltQ,
               temp_CV, DOY, W_air_7_avg) %>% rename(wy_index=Index)

#bdeltLev*deltLev + bQCV*Q_CV + bdeltQ*deltQ + btCV*temp_CV +bDOY*DOY + bAir7*W_air_7_avg,
#      c(bLAG, bt7, bwyi, bdeltLev, bQCV, bdeltQ, btCV, bDOY, bAir7) ~ dnorm(0,5)

# now add other stuff
m1.2 <- map2stan(
    alist(
      breed ~ dbinom( 1 , p ) ,
      logit(p) <- a + a_site[site] + bLAG*lagEM + bt7mx*temp_7_max + bwyi*wy_index + 
      a ~ dnorm(0,5),
      a_site[site] ~ dnorm(0, sigma_site),
      sigma_site ~ dcauchy(0,1),
      c(bLAG, bt7, bwyi) ~ dnorm(0,5)
    ),
    data=d1.2)

precis(m10.2)
logistic(c(1.63, 0.96,-.94, -1.01, 0.80)) # about a 20% chance of breeding based 

m10.2stan <- map2stan( m10.2 , data=d.1s , iter=1e4 , warmup=1000 )
precis(m10.2stan)
plot(m10.2stan)
plot(precis(m10.2stan))



#########
# from Jacob 05/17/17
mz43 <- map2stan(
  alist(
    DOWY ~ dgampois(mu,scale),
    log(mu) <- a_river[river_id] + bt7*temp_7_avg + bwyi*Index + bdeltLev * deltLev +
      bQCV * Q_CV + bdeltQ * deltQ + bQ*Q_cfs + bWair*W_air_30_max + btCV*temp_CV,
    a_river[river_id] ~ dcauchy(10,sigma_site),
    sigma_site ~ dcauchy(1,1),
    c(bt7, bwyi, bdeltLev, bQCV, bdelQ, bQ, bWair, btCV) ~ dnorm(0,1),
    scale ~ dcauchy(0,2)
  ),
  data=d,
  constraints=list(scale="lower=0"),
  start=list(scale=2)
)

```


### Geometric Distribution

The geometric distribution might make some sense (Ch 10, pg 328), as it describes the number of events up until something happens (such as an event, like the start of breeding). If we want to know the probability of that event (often called *Event History Analysis* or *Survival Analysis*), we can use the `dgeom` distribution in R for our common likelihood function. 

The only caveat is that we need to **assume** the probability of the terminating event is constant through time (or distance), and the units of time (or distance) are discrete.

```{r geometric distrib}
# simulate
N <- 100
x <- runif(N)
y <- rgeom( N , prob=logistic( -1 + 2*x ) )

# estimate
m10.18 <- map(
    alist(
        y ~ dgeom( p ),
        logit(p) <- a + b*x,
        a ~ dnorm(0,10),
        b ~ dnorm(0,1)
),
    data=list(y=y,x=x) )
precis(m10.18)
plot(precis(m10.18))

```

## LASSO glm 

```{r lasso}

library(glmnet)

# Only UNREG
d.1 <- dplyr::select(rabo_logist, DOWY, everything(), -date, -EM_per_km, -REG,
                     -(obs_strt:apr_jul), -len_km, -starts_with("CDEC"), 
                     -station, -DOY, -WYsum, -lev_7_avg, -Q_cfs, 
                     -Q_min, -Q_7_cfs) %>% 
  filter(site=="NFY" | site=="NFA") %>% 
  as.data.frame

# RM NAs, add id and site to rownames:
d.1 <- d.1 %>% filter(!is.na(temp_30_min))
d.1_rownames<- paste0(d.1$site, "-",d.1$WY, "-", seq(1:nrow(d.1)))
d.1 <- select(d.1, -site, -WY)
d.1$breed <-as.factor(d.1$breed)
d.1$lagEM <- as.factor(d.1$lagEM)

# make the model matrix for binomial
x <- model.matrix(breed~., data=d.1)
x <- x[,-1] # remove the response var column

# model matrix for gaussian
xG<- d.1 %>% filter(lagEM=="d0") %>% select(-breed, -lagEM)
x2 <- model.matrix(DOWY~., data=xG)
x2 <- x2[,-1] # remove the response var column

# MODEL: BINOMIAL
fit = glmnet(x=x, y=d.1$breed, family = "binomial")
plot(fit, label=T)
print(fit)
coef(fit,s=0.05)

# MODEL: GAUSSIAN
fit2 = glmnet(x=x2, y=xG$DOWY)
plot(fit2, label=T)
print(fit2)
coef(fit2,s=0.05)

# create matrix for prediction (rows, cols)
nx = matrix(rnorm(nrow(x2)*ncol(x2)),nrow(x2),ncol(x2))
predict(fit2,newx=nx,s=c(0.1,0.05)) # new predictions

# cross validated vs. breed
cvfit <- cv.glmnet(x=x2, y=xG$DOWY)
plot(cvfit)
cvfit$lambda.min
coef(cvfit, s=0.05)
coef(cvfit, s="lambda.min")

```

lamda.min is the value of λλ that gives minimum mean cross-validated error. The other λλ saved is lambda.1se, which gives the most regularized model such that error is within one standard error of the minimum. To use that, we only need to replace lambda.min with lambda.1se above

```{r}

newMod <-predict(cvfit, newx = x2, s = "lambda.min") %>% data.frame()
colnames(newMod)<-"predictions"
newMod$siteID<- d.1_rownames[1:12]

ggplot(newMod) + geom_point(aes(x=siteID, y=predictions), pch=16) +
  coord_flip() + ylab("Prob of Breeding") + xlab("")

```


## Logistic `glm` Model

Now let's run a model. See [here](http://stats.idre.ucla.edu/r/dae/logit-regression/)

```{r model}

# Only UNREG
xG<- d.1 %>% filter(lagEM=="d0") %>% select(-breed, -lagEM)

# All sites
d.2 <- dplyr::select(rabo_logist, DOWY, everything(), -date, -Index, -WYsum,
                     -EM_per_km, -station, -(obs_strt:apr_jul), 
                     -len_km, -starts_with("CDEC"), -DOY, -WYsum,
                     -Q_min, -Q_7_cfs, -lev_7_avg) %>%
  as.data.frame

# fix index
#d.2$breed <-as.integer(as.factor(d.2$breed))
d.2$lagEM <- as.integer(as.factor(d.2$lagEM))
d.2$site <- as.integer(as.factor(d.2$site))
d.2$REG <- as.integer(as.factor(d.2$REG))

# scaled
d.2.s<-scale(d.2) %>% as.data.frame


# run a gaussian response model (unscaled)
mod.gauss <- glm(DOWY ~ deltQ + Q_CV + Q_7_CV +
                   W_air_7_avg + W_air_30_max + W_humidity_avg +
                   temp_7_avg + temp_7_max + temp_30_min + temp_30_max +
                   temp_CV + lev_CV + deltLev + Index +
                   days_no_ppt, data = xG, family = "gaussian")
summary(mod.gauss)
anova(mod.gauss)

# run a logistic binomial response model (unscaled)
mod.logit <- glm(breed ~ deltQ + Q_CV + Q_7_CV +
                 W_air_7_avg + W_air_30_max + W_humidity_avg +
                 temp_7_avg + temp_7_max + temp_30_min + temp_30_max +
                 temp_CV + lev_CV + deltLev +
                 days_no_ppt + site, data = d.2, family = "binomial")
summary(mod.logit)
anova(mod.logit)
```

So **Deviance residuals** are a measure of model fit, and `AIC = 178.8`. For every one unit change in **`temp_7_max`** there is a log-odds of oviposition timing (breeding) increase of 20.2224, whereas a change in the stage (`lev_CV`) yields a strong negative change in the log odds of oviposition by -2.09

You can calculate the confidence intervals using the `confint` function.

```{r}
confint(mod.logit)

```

```{r}
# simulate and predict

newdata1 <- d %>% select(DOWY, site, lev_avg:deltQ) %>% group_by(site,  DOY) #%>% 
  summarize(airmin.7 = mean(airmin.7),
            days_wo_ppt = mean(days_wo_ppt),
            level.avg = mean(level.avg),
            temp.min.7 = mean(temp.min.7),
            level.7dL = mean(level.7dL))

# predict
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
ggplot() + geom_point(data=newdata1, aes(x=DOY, y=rankP, color=as.factor(river_id), shape=as.factor(WY_id)), size=3)
```




## BRT

```{r brt}
library(gbm)
library(dismo)
library(ggplot2)
library(viridis)

set.seed(33)

# gbm First Model -----
d.1brt <- dplyr::select(rabo_logist, DOWY, everything(), 
                     -date, -EM_per_km, -REG,
                     -(obs_strt:apr_jul), -len_km,
                     -starts_with("CDEC"), 
                     #-starts_with("W_"), 
                     -station, -DOY, -WYsum, 
                     -lev_7_avg, -Q_cfs, 
                     -Q_min, -Q_max, -Q_7_cfs) %>% 
  filter(site=="NFY" | site=="NFA") %>% 
  filter(!is.na(temp_30_min)) %>% 
  dplyr::select(breed, everything(), -Index, -lev_avg, -DOWY) %>%
  as.data.frame()
d.1brt$site <- as.factor(d.1brt$site)
d.1brt$lagEM <- as.factor(d.1brt$site)

gbm3s <- gbm.step(data = d.1brt,
                     gbm.x = 2:26,    
                     gbm.y = 1,
                     family = "bernoulli",  
                     tree.complexity = 2,   
                     learning.rate = 0.001, 
                     bag.fraction = 0.75,   
                     n.folds = 5,      
                     plot.main=T,
                     verbose=T) 

paste0("mean estimated deviance from CV: ", round(gbm3s$cv.statistics$deviance.mean,3))
paste0("SE estimated deviance from CV: ", round(gbm3s$cv.statistics$deviance.se,3))

# gbm simplify barplot ----

par(mar=c(5,12,3,3))
barplot(rev(summary(gbm3s, plotit=FALSE)$rel.inf), 
        horiz = TRUE, col = viridis(length(gbm3s$var.names)), 
        names = rev(summary(gbm3s, plotit=FALSE)$var), 
        xlab = "Relative influence",
        las=1, 
        main="Relative Influence")

topVars<-summary(gbm3s, plot=F) %>% as.tibble %>% filter(rel.inf>4)
topVars


## Another BRT but simplified
names(d.1brt)
topVars$var <- factor(topVars$var)
d.1simp <- dplyr::select(d.1brt, breed, site, deltQ, W_air_30_max,
                         temp_30_min, temp_7_max, deltLev,
                         W_humidity_avg, lev_CV)

gbm3s <- gbm.step(data = d.1simp,
                     gbm.x = 2:9,    
                     gbm.y = 1,
                     family = "bernoulli",  
                     tree.complexity = 2,   
                     learning.rate = 0.001, 
                     bag.fraction = 0.75,   
                     n.folds = 5,      
                     plot.main=T,
                     verbose=T) 

paste0("mean estimated deviance from CV: ", round(gbm3s$cv.statistics$deviance.mean,3))
paste0("se estimated deviance from CV: ", round(gbm3s$cv.statistics$deviance.se,3))

# gbm simplify barplot ----

par(mar=c(5,12,3,3))
barplot(rev(summary(gbm3s, plotit=FALSE)$rel.inf), 
        horiz = TRUE, col = viridis(length(gbm3s$var.names)), 
        names = rev(summary(gbm3s, plotit=FALSE)$var), 
        xlab = "Relative influence",
        las=1, 
        main="Relative Influence")

summary(gbm3s, plot=F)
```

