---
title: "Oviposition Logistic"
author: "Ryan Peek"
date: "Updated: `r format(Sys.Date())`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = normalizePath("../"))
```

## Try Model, Model Fail

Let's try to assess whether water temperature, air temperature, flow (daily/weekly recession rates & discharge) play a role in predicting when breeding (oviposition occurs). To build this model we need to block by river, and by water year, as these are independent (though water year isn't *fixed*, it is fixed across all sites per each year).

First let's take a look at some test data. Using a subset (say just pull the **NF American**), can we build a model that gets at whether there is predictability in these variables despite shifts in timing and magnitude from year to year?

### Get the Data & Tidy

```{r get and tidy data}
library(tidyverse)
library(lubridate)

load("data/master_dat_2011-2016") 
load("data/daily_flow_cfs_data_6sites.rda") # updated and merged flows:

# sites <- c("NFA", "NFY") # if you want to restrict the sites

# site lengths for EM/km adj
sites<- tibble("site"=c("RUB","NFA"," NFA-RR", "MFA","MFA-AMC", "SFY", "NFY", "MFY"), "len_km"=c(0.5, 0.57, 0.11, 0.5, 0.2, 0.5, 0.8, 0.8))

# join site-lengths w master 
master_df <- master_df %>%
  left_join(., sites, by="site") %>% 
  mutate(EM_per_km=totalEM/len_km)

save(master_df, file = "./data/master_dat_2011-2016.rda")

#df <- master_df
# df <- master_df %>% filter(site!="MFY")
df <- master_df %>% filter(month(date)>3 & month(date)<8)

# 2 NAs in dataset from 30 day avgs
df[!is.na(df$missData) & df$site=="RUB" & df$date==ymd("2011-05-26"),]$temp_30_max<-10.99
df[!is.na(df$missData) & df$site=="RUB" & df$date==ymd("2011-05-26"),]$temp_30_min<-9.7

rabo <- df[!is.na(df$missData),] %>% arrange(date)

```

Select pieces you'll need:

```{r tidy_data}
library(lubridate)

rabo <- df[!is.na(df$missData),] %>% arrange(date)

# refactor the sites to drop unused factors
rabo$site <- factor(rabo$site)

# drop factors
rabo <- rabo %>%
  mutate("siteID" = paste0(site, "-", row.names(.))) %>% 
  column_to_rownames(var = "siteID")  %>% as.data.frame
names(rabo)
rownames(rabo)

d <- rabo %>% dplyr::select(-(REG:apr_jul), -starts_with("CDEC"), -station, -WY, -DOY)


```

## PCA

Try a PCA of the variables to see which is strongest predictor and potentially reduce collinearity.

```{r pca}

# need to create a train set:
train <- d2 %>% sample_n(size=300, replace=FALSE) %>% row.names() %>% as.integer()

traindat <- d2[train,]
testdat <- d2[-train,]

#remove the dependent and identifier variables
traindat <- traindat %>% select(-ovipos)

# PCA
prin_comp <- prcomp(traindat, scale. = T)
prin_comp$center

# rotation: The rotation measure provides the principal component loading. Each column of rotation matrix contains the principal component loading vector. This is the most important measure we should be interested in
prin_comp$rotation

# summary of components
summary(prin_comp) # so PC1 : PC3 explain 83% of the total variance in the data

# loadings per variable PC1
prin_comp$rotation[,1:3] %>% as.data.frame %>%
  mutate(vars = row.names(prin_comp$rotation)) %>% 
  select(vars, PC1) %>% arrange(PC1)

# loadings per variable PC2 & PC3
prin_comp$rotation[,2:3] %>% as.data.frame %>%
  mutate(vars = row.names(prin_comp$rotation)) %>% 
  select(vars, PC2) %>% arrange(PC2)

# loadings per variable PC3
prin_comp$rotation[,1:3] %>% as.data.frame %>%
  mutate(vars = row.names(prin_comp$rotation)) %>% 
  select(vars, PC3) %>% arrange(PC3)


# plot
biplot(prin_comp, choices = 1:2, scale = 0, pc.biplot = F, xlabs=rep(".", nrow(traindat)))
biplot(prin_comp, choices = 2:3, scale = 0, pc.biplot = F, xlabs=rep(".", nrow(traindat)))
dev.off()

# a quad plot of results

# THE FUNCTION:
pcaCharts <- function(x) {
    x.var <- x$sdev ^ 2
    x.pvar <- x.var/sum(x.var)
    print("proportions of variance:")
    print(x.pvar)
    
    par(mfrow=c(2,2))
    plot(x.pvar,xlab="Principal component", ylab="Proportion of variance explained", ylim=c(0,1), type='b')
    plot(cumsum(x.pvar),xlab="Principal component", ylab="Cumulative Proportion of variance explained", ylim=c(0,1), type='b')
    screeplot(x)
    screeplot(x,type="l")
    par(mfrow=c(1,1))
}

# plot
pcaCharts(prin_comp)

```



## BRT Model

```{r brt1}
library(dismo)

# get data
summary(rabo)
d <- rabo %>% dplyr::select(-(REG:apr_jul), -starts_with("CDEC"), -station, -WY, -DOY)
summary(d)
names(d)

# fix indexes
d$DOWY <- as.integer(d$DOWY)

# drop factors
d <- dplyr::select(d, DOWY, everything(), -date) %>%  as.data.frame
names(d)

egg.tc5.lr01 <- gbm.step(data= d, gbm.x = 2:23, gbm.y = 1,
                         family = "poisson", tree.complexity = 3,
                         learning.rate = 0.0005, bag.fraction = 0.7)

summary(egg.tc5.lr01)

# plot
gbm.plot(egg.tc5.lr01, rug = T, n.plots = 12, write.title = FALSE)
gbm.plot.fits(egg.tc5.lr01)

# interactions
find.int <- gbm.interactions(egg.tc5.lr01)
find.int$interactions
find.int$rank.list

# plot pairwise interactions:
gbm.perspec(egg.tc5.lr01, 7, 5)#y.range=c(15,20), z.range=c(0,0.6))
```


```{r brt2}

library(dismo)
tst<- read.csv("./data/HSC/egg_use_availability_equal_samples.csv")
tst2<- read.csv("./data/HSC/Eggs_HSC_ncss_export.csv") %>% 
  mutate(Date= as.Date(Date,  origin="1899-12-30", tz="America/Los_Angeles")) %>% 
  select(-starts_with("Random"))
  #mutate(Date= as.POSIXct(Date * (60*60*24),  origin="1899-12-30", tz="America/Los_Angeles")) # for datetime
head(tst2)

head(tst)
tst1 <- tst %>% sample_n(size=200, replace=FALSE) # create training set

egg.tc5.lr01 <- gbm.step(data=tst1, gbm.x = 3:5, gbm.y = 1,
                         family = "bernoulli", tree.complexity = 5,
                         learning.rate = 0.01, bag.fraction = 0.5)

summary(egg.tc5.lr01)

# plot
gbm.plot(egg.tc5.lr01, rug = T, n.plots = 8, write.title = FALSE)
gbm.plot.fits(egg.tc5.lr01)

# interactions
find.int <- gbm.interactions(egg.tc5.lr01)
find.int$interactions
find.int$rank.list

# plot pairwise interactions:
plot_ly(gbm.perspec(egg.tc5.lr01, 1, 2))#y.range=c(15,20), z.range=c(0,0.6))

# alternate version with use vs. available
tst2$UseBinary <- ifelse(tst2$Data_Type=="USE", 1, 0)
tst2_brt <- select(tst2, UseBinary, 10, 5:8, 11:23) %>% 
  filter(!is.na(Sub_Cont_6),
         !Meso_Type=="")

egg2.tc5.lr01 <- gbm.step(data=tst2_brt, gbm.x = 2:19, gbm.y = 1,
                         family = "bernoulli", tree.complexity = 5,
                         learning.rate = 0.01, bag.fraction = 0.5)

summary(egg2.tc5.lr01)

# plot
gbm.plot(egg2.tc5.lr01, rug = T, n.plots = 12, write.title = FALSE)
gbm.plot.fits(egg2.tc5.lr01)

# interactions
find.int <- gbm.interactions(egg2.tc5.lr01)
find.int$interactions
find.int$rank.list

# plot pairwise interactions:
gbm.perspec(egg2.tc5.lr01, 5,4)#y.range=c(15,20), z.range=c(0,0.6))


```

## Logistic `glm` Model

Now let's run a model. See [here](http://stats.idre.ucla.edu/r/dae/logit-regression/)

```{r model}

d <- rabo %>% dplyr::select(-(obs_strt:apr_jul), -starts_with("CDEC"), -station, -WY, -DOY)

# fix index
d$river <- as.integer(as.factor(d$site))
d$REG <- as.integer(as.factor(d$REG))

# xtabs(~DOWY + site, data = d)

# run a logistic binomial response model
mylogit <- glm(DOWY ~ river + REG  + deltQ + Q_CV + Index + W_air_7_avg + temp_7_avg + temp_30_min + W_humidity_avg + temp_30_max + temp_30_min + days_no_ppt + temp_CV + temp_7_max + deltLev + lev_CV + lev_avg + W_air_30_max, data = d, family = "gaussian")
summary(mylogit)

```

So **Deviance residuals** are a measure of model fit, and `AIC = 178.8`. For every one unit change in **`temp_7_max`** there is a log-odds of oviposition timing (breeding) increase of 20.2224, whereas a change in the stage (`lev_CV`) yields a strong negative change in the log odds of oviposition by -2.09

You can calculate the confidence intervals using the `confint` function.

```{r}
confint(mylogit)

library(aod)
# Can use a wald.test to test coefficients in order they appear in model
aod::wald.test(b= coef(mylogit), Sigma = vcov(mylogit), Terms=c(1,10))
aod::wald.test(b= coef(mylogit), Sigma = vcov(mylogit), Terms=c(1,13))

## odds ratios and 95% CI
exp(cbind(OR = coef(mylogit), confint(mylogit)))

```

```{r}
# simulate and predict

newdata1 <- d %>% select(DOWY, site, lev_avg:deltQ) %>% group_by(site,  DOY) #%>% 
  summarize(airmin.7 = mean(airmin.7),
            days_wo_ppt = mean(days_wo_ppt),
            level.avg = mean(level.avg),
            temp.min.7 = mean(temp.min.7),
            level.7dL = mean(level.7dL))

# predict
newdata1$rankP <- predict(mylogit, newdata = newdata1, type = "response")
newdata1
ggplot() + geom_point(data=newdata1, aes(x=DOY, y=rankP, color=as.factor(river_id), shape=as.factor(WY_id)), size=3)
```



## Rethinking Model

```{r varying effects, eval=F, echo=T}
library(rethinking)

# fix index
d$river <- as.integer(as.factor(d$site))

# from Jacob 05/17/17
mz43 <- map2stan(
  alist(
    DOWY ~ dgampois(mu,scale),
    log(mu) <- a_river[river_id] + bt7*temp_7_avg + bwyi*Index + bdeltLev * deltLev +
      bQCV * Q_CV + bdeltQ * deltQ + bQ*Q_cfs + bWair*W_air_30_max + btCV*temp_CV,
    a_river[river_id] ~ dcauchy(10,sigma_site),
    sigma_site ~ dcauchy(1,1),
    c(bt7, bwyi, bdeltLev, bQCV, bdelQ, bQ, bWair, btCV) ~ dnorm(0,1),
    scale ~ dcauchy(0,2)
  ),
  data=d,
  constraints=list(scale="lower=0"),
  start=list(scale=2)
)

```
